{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Data Loading - Star Schema for Business Intelligence\n",
    "\n",
    "This notebook loads cleaned data into a SQL database using a **Star Schema** dimensional model optimized for BI and analytics.\n",
    "\n",
    "## Architecture:\n",
    "```\n",
    "Cleaned CSVs ‚Üí Star Schema (SQLite)\n",
    "                  ‚îú‚îÄ‚îÄ Fact: fact_sales\n",
    "                  ‚îî‚îÄ‚îÄ Dimensions:\n",
    "                      ‚îú‚îÄ‚îÄ dim_customer\n",
    "                      ‚îú‚îÄ‚îÄ dim_product\n",
    "                      ‚îú‚îÄ‚îÄ dim_date\n",
    "                      ‚îú‚îÄ‚îÄ dim_geography\n",
    "                      ‚îî‚îÄ‚îÄ dim_order\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Setup & Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env from: /Users/diegoferra/Documents/Python codes/bloque_clase/notebooks/../.env\n",
      "File exists: True\n",
      "\n",
      "Loaded credentials:\n",
      "Host: aws-1-us-east-1.pooler.supabase.com\n",
      "User: postgres.ypznufmiuekmrtdjmcux\n",
      "Port: 6543\n",
      "Password: *********\n",
      "\n",
      "Testing connection...\n",
      "‚úÖ Connected to Supabase PostgreSQL!\n",
      "Database version: PostgreSQL 17.6 on aarch64-unknown-linux-gnu, compiled by gcc (GCC) 13.2.0, 64-b...\n"
     ]
    }
   ],
   "source": [
    "# 01. Setup & Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Pandas display configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Load environment variables from project root\n",
    "# Find the .env file in the parent directory (project root)\n",
    "env_path = Path('..') / '.env'\n",
    "\n",
    "# IMPORTANT: override=True forces reload even if variables already exist\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "print(f\"Loading .env from: {env_path.absolute()}\")\n",
    "print(f\"File exists: {env_path.exists()}\")\n",
    "\n",
    "# Supabase Database Configuration\n",
    "DATABASE_URL = os.getenv('DATABASE_URL')\n",
    "\n",
    "# Alternative: Build connection string from individual components\n",
    "if not DATABASE_URL:\n",
    "    SUPABASE_HOST = os.getenv('SUPABASE_HOST')\n",
    "    SUPABASE_PORT = os.getenv('SUPABASE_PORT', '5432')\n",
    "    SUPABASE_DATABASE = os.getenv('SUPABASE_DATABASE', 'postgres')\n",
    "    SUPABASE_USER = os.getenv('SUPABASE_USER')\n",
    "    SUPABASE_PASSWORD = os.getenv('SUPABASE_PASSWORD')\n",
    "    \n",
    "    # Debug: Check if variables are loaded\n",
    "    print(f\"\\nLoaded credentials:\")\n",
    "    print(f\"Host: {SUPABASE_HOST}\")\n",
    "    print(f\"User: {SUPABASE_USER}\")\n",
    "    print(f\"Port: {SUPABASE_PORT}\")\n",
    "    print(f\"Password: {'*' * len(SUPABASE_PASSWORD) if SUPABASE_PASSWORD else 'NOT SET'}\")\n",
    "    \n",
    "    if not all([SUPABASE_HOST, SUPABASE_USER, SUPABASE_PASSWORD]):\n",
    "        raise ValueError(\"‚ùå Missing Supabase credentials in .env file!\")\n",
    "    \n",
    "    DATABASE_URL = f\"postgresql://{SUPABASE_USER}:{SUPABASE_PASSWORD}@{SUPABASE_HOST}:{SUPABASE_PORT}/{SUPABASE_DATABASE}\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Test connection\n",
    "print(\"\\nTesting connection...\")\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT version();\"))\n",
    "        version = result.fetchone()[0]\n",
    "        print(\"‚úÖ Connected to Supabase PostgreSQL!\")\n",
    "        print(f\"Database version: {version[:80]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   1. Verify your password is correct in .env\")\n",
    "    print(\"   2. Check Supabase dashboard for connection string\")\n",
    "    print(\"   3. Ensure you're using Session Pooler (port 6543)\")\n",
    "    print(\"   4. Try restarting the Jupyter kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Loading cleaned datasets from data/processed/\n",
      "\n",
      "üìä DATASETS LOADED SUCCESSFULLY\n",
      "======================================================================\n",
      "Customer Address         : 221,437 rows √ó  26 cols | 361.24 MB\n",
      "Individual Customer      : 178,494 rows √ó  52 cols | 386.59 MB\n",
      "Product Catalog          :   7,158 rows √ó   6 cols |   1.95 MB\n",
      "Orders List              :  67,831 rows √ó  38 cols | 170.59 MB\n",
      "General Order            :  59,310 rows √ó  43 cols | 105.25 MB\n",
      "Product Order Detail     :  87,609 rows √ó 108 cols | 256.62 MB\n",
      "======================================================================\n",
      "TOTAL                    : 621,839 rows             | 1282.23 MB\n",
      "======================================================================\n",
      "\n",
      "‚úÖ All datasets ready for transformation into Star Schema!\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned datasets with low_memory=False to avoid dtype warnings\n",
    "print(\"\\nüìÇ Loading cleaned datasets from data/processed/\\n\")\n",
    "\n",
    "customerAddress = pd.read_csv('../data/processed/clean_CustomerAddress.csv', low_memory=False)\n",
    "individualCustomer = pd.read_csv('../data/processed/clean_IndividualCustomer.csv', low_memory=False)\n",
    "productCatalog = pd.read_csv('../data/processed/clean_ProductCatalog.csv', low_memory=False)\n",
    "ordersList = pd.read_csv('../data/processed/clean_OrdersList.csv', low_memory=False)\n",
    "generalOrder = pd.read_csv('../data/processed/clean_GeneralOrderDetail.csv', low_memory=False)\n",
    "productOrderDetail = pd.read_csv('../data/processed/clean_ProductOrderDetail.csv', low_memory=False)\n",
    "\n",
    "# Verify datasets loaded successfully\n",
    "datasets = {\n",
    "    'Customer Address': customerAddress,\n",
    "    'Individual Customer': individualCustomer,\n",
    "    'Product Catalog': productCatalog,\n",
    "    'Orders List': ordersList,\n",
    "    'General Order': generalOrder,\n",
    "    'Product Order Detail': productOrderDetail\n",
    "}\n",
    "\n",
    "print(\"üìä DATASETS LOADED SUCCESSFULLY\")\n",
    "print(\"=\" * 70)\n",
    "for name, df in datasets.items():\n",
    "    memory_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"{name:25s}: {len(df):>7,} rows √ó {len(df.columns):>3} cols | {memory_mb:>6.2f} MB\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate total statistics\n",
    "total_rows = sum(len(df) for df in datasets.values())\n",
    "total_memory = sum(df.memory_usage(deep=True).sum() / 1024**2 for df in datasets.values())\n",
    "print(f\"{'TOTAL':25s}: {total_rows:>7,} rows             | {total_memory:>6.2f} MB\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úÖ All datasets ready for transformation into Star Schema!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Star Schema Design\n",
    "\n",
    "## Dimensional Model Overview\n",
    "\n",
    "The star schema consists of one **fact table** surrounded by **dimension tables**. This design optimizes query performance for analytical workloads and BI dashboards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Fact Table: `fact_sales`\n",
    "\n",
    "**Grain:** One row per product item in an order\n",
    "\n",
    "**Purpose:** Stores transactional sales data with foreign keys to dimensions\n",
    "\n",
    "### Schema:\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `sale_id` | INTEGER PRIMARY KEY | Surrogate key (auto-increment) |\n",
    "| `order_id` | VARCHAR(50) | Business key - Order identifier |\n",
    "| `customer_key` | INTEGER FK | ‚Üí dim_customer |\n",
    "| `product_key` | INTEGER FK | ‚Üí dim_product |\n",
    "| `date_key` | INTEGER FK | ‚Üí dim_date (YYYYMMDD format) |\n",
    "| `geography_key` | INTEGER FK | ‚Üí dim_geography |\n",
    "| `order_key` | INTEGER FK | ‚Üí dim_order |\n",
    "| **Measures (Metrics):** | | |\n",
    "| `quantity` | INTEGER | Units sold |\n",
    "| `unit_price` | DECIMAL(10,2) | Price per unit |\n",
    "| `list_price` | DECIMAL(10,2) | Original list price |\n",
    "| `selling_price` | DECIMAL(10,2) | Final selling price |\n",
    "| `discount_amount` | DECIMAL(10,2) | Discount applied |\n",
    "| `shipping_price` | DECIMAL(10,2) | Shipping cost |\n",
    "| `total_amount` | DECIMAL(10,2) | Total transaction value |\n",
    "| `is_gift` | BOOLEAN | Gift flag |\n",
    "\n",
    "**Source Tables:** `productOrderDetail` (primary), `ordersList`, `generalOrder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë§ Dimension: `dim_customer`\n",
    "\n",
    "**Purpose:** Customer profile and demographic information\n",
    "\n",
    "### Schema:\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `customer_key` | INTEGER PRIMARY KEY | Surrogate key |\n",
    "| `user_id` | VARCHAR(50) UNIQUE | Business key |\n",
    "| `birth_date` | DATE | Date of birth |\n",
    "| `customer_age` | INTEGER | Calculated age |\n",
    "| `gender` | VARCHAR(10) | Customer gender |\n",
    "| `email` | VARCHAR(255) | Email address |\n",
    "| `phone` | VARCHAR(50) | Phone number |\n",
    "| `first_purchase_date` | DATE | Date of first purchase |\n",
    "| `last_session_date` | DATETIME | Last platform activity |\n",
    "| `is_active` | BOOLEAN | Active customer flag |\n",
    "| `created_at` | DATETIME | Record creation timestamp |\n",
    "\n",
    "**Source Table:** `individualCustomer`\n",
    "\n",
    "**SCD Type:** Type 1 (overwrite) - For this project, we assume customer data doesn't need historical tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõí Dimension: `dim_product`\n",
    "\n",
    "**Purpose:** Product catalog and hierarchy information\n",
    "\n",
    "### Schema:\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `product_key` | INTEGER PRIMARY KEY | Surrogate key |\n",
    "| `product_id` | VARCHAR(50) UNIQUE | Business key (IdMaterial) |\n",
    "| `product_name` | VARCHAR(255) | Product material name |\n",
    "| `ean_upc` | VARCHAR(50) | Barcode |\n",
    "| `brand` | VARCHAR(100) | Product brand |\n",
    "| `category` | VARCHAR(100) | Product category |\n",
    "| `segment` | VARCHAR(100) | Product segment |\n",
    "| `is_active` | BOOLEAN | Active in catalog |\n",
    "\n",
    "**Source Table:** `productCatalog`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÖ Dimension: `dim_date`\n",
    "\n",
    "**Purpose:** Time intelligence for temporal analysis\n",
    "\n",
    "### Schema:\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `date_key` | INTEGER PRIMARY KEY | YYYYMMDD format (e.g., 20210115) |\n",
    "| `full_date` | DATE UNIQUE | Actual date |\n",
    "| `year` | INTEGER | Year (2021, 2022) |\n",
    "| `quarter` | INTEGER | Quarter (1-4) |\n",
    "| `month` | INTEGER | Month (1-12) |\n",
    "| `month_name` | VARCHAR(20) | Month name (January, etc.) |\n",
    "| `week_of_year` | INTEGER | ISO week number |\n",
    "| `day_of_month` | INTEGER | Day (1-31) |\n",
    "| `day_of_week` | INTEGER | Weekday (1=Monday, 7=Sunday) |\n",
    "| `day_name` | VARCHAR(20) | Day name (Monday, etc.) |\n",
    "| `is_weekend` | BOOLEAN | Weekend flag |\n",
    "| `is_holiday` | BOOLEAN | Holiday flag (optional) |\n",
    "| `quarter_name` | VARCHAR(10) | Q1, Q2, Q3, Q4 |\n",
    "| `year_month` | VARCHAR(10) | YYYY-MM format |\n",
    "\n",
    "**Source:** Generated programmatically from date range in data (Jan 2021 - Nov 2022)\n",
    "\n",
    "**Note:** This is a conformed dimension - same date dimension used across all facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìç Dimension: `dim_geography`\n",
    "\n",
    "**Purpose:** Location and address information for geographic analysis\n",
    "\n",
    "### Schema:\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `geography_key` | INTEGER PRIMARY KEY | Surrogate key |\n",
    "| `address_id` | VARCHAR(50) | Business key |\n",
    "| `user_id` | VARCHAR(50) | Associated customer |\n",
    "| `country` | VARCHAR(100) | Country name |\n",
    "| `state` | VARCHAR(100) | State/province |\n",
    "| `city` | VARCHAR(100) | City name |\n",
    "| `neighborhood` | VARCHAR(100) | Neighborhood |\n",
    "| `postal_code` | VARCHAR(20) | ZIP/postal code |\n",
    "| `street` | VARCHAR(255) | Street address |\n",
    "| `latitude` | DECIMAL(10,8) | Geographic coordinate |\n",
    "| `longitude` | DECIMAL(11,8) | Geographic coordinate |\n",
    "| `address_type` | VARCHAR(50) | Residential, commercial, etc. |\n",
    "| `is_default` | BOOLEAN | Default address flag |\n",
    "\n",
    "**Source Table:** `customerAddress`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Dimension: `dim_order`\n",
    "\n",
    "**Purpose:** Order-level attributes and status information\n",
    "\n",
    "### Schema:\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `order_key` | INTEGER PRIMARY KEY | Surrogate key |\n",
    "| `order_id` | VARCHAR(50) UNIQUE | Business key |\n",
    "| `creation_date` | DATETIME | Order creation timestamp |\n",
    "| `authorized_date` | DATETIME | Payment authorization |\n",
    "| `invoiced_date` | DATETIME | Invoice date |\n",
    "| `order_status` | VARCHAR(50) | Current status |\n",
    "| `payment_method` | VARCHAR(50) | Payment type |\n",
    "| `shipping_estimated_date` | DATE | Estimated delivery |\n",
    "| `shipping_estimated_min` | DATE | Min delivery estimate |\n",
    "| `shipping_estimated_max` | DATE | Max delivery estimate |\n",
    "| `days_to_shipping` | INTEGER | Days from order to ship |\n",
    "| `order_year` | INTEGER | Order year |\n",
    "| `order_month` | INTEGER | Order month |\n",
    "| `order_quarter` | INTEGER | Order quarter |\n",
    "| `order_day_of_week` | INTEGER | Order weekday |\n",
    "| `channel` | VARCHAR(50) | Sales channel |\n",
    "| `seller_id` | VARCHAR(50) | Seller identifier |\n",
    "\n",
    "**Source Tables:** `ordersList`, `generalOrder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Relationships & Cardinality\n",
    "\n",
    "```\n",
    "dim_customer (1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (*) fact_sales\n",
    "dim_product (1)  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (*) fact_sales\n",
    "dim_date (1)     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (*) fact_sales\n",
    "dim_geography (1)‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (*) fact_sales\n",
    "dim_order (1)    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (*) fact_sales\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- All relationships are **1:Many** (dimension ‚Üí fact)\n",
    "- Fact table contains **only foreign keys + measures**\n",
    "- Dimensions are **denormalized** for query performance\n",
    "- Date dimension is **pre-populated** with all dates in range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Design Decisions & Notes\n",
    "\n",
    "### 1. Grain Selection\n",
    "- **Fact grain:** Product line item per order (most atomic level)\n",
    "- Allows aggregation to any level: order, customer, product, day, etc.\n",
    "\n",
    "### 2. Surrogate Keys\n",
    "- All dimensions use auto-increment surrogate keys\n",
    "- Business keys (userId, orderId, productId) preserved for reference\n",
    "- Simplifies joins and improves performance\n",
    "\n",
    "### 3. Slowly Changing Dimensions (SCD)\n",
    "- **Type 1 (Overwrite)** for all dimensions\n",
    "- No historical tracking needed for this project\n",
    "- Future enhancement: Type 2 for customer/product changes\n",
    "\n",
    "### 4. Degenerate Dimensions\n",
    "- `order_id` stored in fact table (not just FK)\n",
    "- Allows grouping by order without joining dim_order\n",
    "\n",
    "### 5. Conformed Dimensions\n",
    "- `dim_date` is a conformed dimension\n",
    "- Can be reused across multiple fact tables if schema expands\n",
    "\n",
    "### 6. Missing Data Handling\n",
    "- Unknown/missing dimension values ‚Üí special record with key = -1\n",
    "- Example: Unknown customer, Unknown product, etc.\n",
    "\n",
    "### 7. Data Types\n",
    "- Decimals for monetary values (avoid floating point errors)\n",
    "- VARCHAR with appropriate lengths\n",
    "- DATE/DATETIME for temporal columns\n",
    "- BOOLEAN for flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Business Metrics Enabled by This Model\n",
    "\n",
    "This star schema design enables analysis of:\n",
    "\n",
    "**Sales Performance:**\n",
    "- Total revenue by period/product/customer\n",
    "- Average order value\n",
    "- Discount effectiveness\n",
    "- Shipping cost analysis\n",
    "\n",
    "**Customer Analytics:**\n",
    "- Customer lifetime value (CLV)\n",
    "- Customer segmentation by age/geography\n",
    "- Repeat purchase rate\n",
    "- Customer acquisition trends\n",
    "\n",
    "**Product Analytics:**\n",
    "- Top products by revenue/quantity\n",
    "- Category performance\n",
    "- Brand comparison\n",
    "- Product mix analysis\n",
    "\n",
    "**Geographic Analytics:**\n",
    "- Sales by country/state/city\n",
    "- Regional performance\n",
    "- Market penetration\n",
    "\n",
    "**Temporal Analytics:**\n",
    "- Seasonality patterns\n",
    "- Year-over-year growth\n",
    "- Weekend vs weekday sales\n",
    "- Monthly/quarterly trends\n",
    "\n",
    "**Operational Metrics:**\n",
    "- Fulfillment time (days to shipping)\n",
    "- Order status distribution\n",
    "- Payment method preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Create SQL Tables (DDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating Star Schema Tables in PostgreSQL\n",
      "\n",
      "Executing DDL statements...\n",
      "\n",
      "  ‚öôÔ∏è  Drop existing tables...\n",
      "     ‚úÖ Drop existing tables - SUCCESS\n",
      "  ‚öôÔ∏è  Create dim_customer...\n",
      "     ‚úÖ Create dim_customer - SUCCESS\n",
      "  ‚öôÔ∏è  Create dim_product...\n",
      "     ‚úÖ Create dim_product - SUCCESS\n",
      "  ‚öôÔ∏è  Create dim_date...\n",
      "     ‚úÖ Create dim_date - SUCCESS\n",
      "  ‚öôÔ∏è  Create dim_geography...\n",
      "     ‚úÖ Create dim_geography - SUCCESS\n",
      "  ‚öôÔ∏è  Create dim_order...\n",
      "     ‚úÖ Create dim_order - SUCCESS\n",
      "  ‚öôÔ∏è  Create fact_sales...\n",
      "     ‚úÖ Create fact_sales - SUCCESS\n",
      "\n",
      "======================================================================\n",
      "üéâ Star Schema created successfully in PostgreSQL!\n",
      "======================================================================\n",
      "\n",
      "üìã Tables created in database:\n",
      "   ‚Ä¢ dim_customer\n",
      "   ‚Ä¢ dim_date\n",
      "   ‚Ä¢ dim_geography\n",
      "   ‚Ä¢ dim_order\n",
      "   ‚Ä¢ dim_product\n",
      "   ‚Ä¢ fact_sales\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Creating Star Schema Tables in PostgreSQL\\n\")\n",
    "\n",
    "# DDL Statements for Star Schema\n",
    "ddl_statements = []\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DROP EXISTING TABLES (if any) - in reverse order due to FK constraints\n",
    "# ==============================================================================\n",
    "drop_tables = \"\"\"\n",
    "DROP TABLE IF EXISTS fact_sales CASCADE;\n",
    "DROP TABLE IF EXISTS dim_customer CASCADE;\n",
    "DROP TABLE IF EXISTS dim_product CASCADE;\n",
    "DROP TABLE IF EXISTS dim_date CASCADE;\n",
    "DROP TABLE IF EXISTS dim_geography CASCADE;\n",
    "DROP TABLE IF EXISTS dim_order CASCADE;\n",
    "\"\"\"\n",
    "\n",
    "ddl_statements.append((\"Drop existing tables\", drop_tables))\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CREATE DIMENSION TABLES\n",
    "# ==============================================================================\n",
    "\n",
    "# --- dim_customer ---\n",
    "create_dim_customer = \"\"\"\n",
    "CREATE TABLE dim_customer (\n",
    "    customer_key SERIAL PRIMARY KEY,\n",
    "    user_id VARCHAR(50) UNIQUE,\n",
    "    birth_date DATE,\n",
    "    customer_age INTEGER,\n",
    "    gender VARCHAR(10),\n",
    "    email VARCHAR(255),\n",
    "    phone VARCHAR(50),\n",
    "    first_purchase_date DATE,\n",
    "    last_session_date TIMESTAMP,\n",
    "    is_active BOOLEAN,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "CREATE INDEX idx_customer_user_id ON dim_customer(user_id);\n",
    "CREATE INDEX idx_customer_age ON dim_customer(customer_age);\n",
    "\"\"\"\n",
    "\n",
    "ddl_statements.append((\"Create dim_customer\", create_dim_customer))\n",
    "\n",
    "# --- dim_product ---\n",
    "create_dim_product = \"\"\"\n",
    "CREATE TABLE dim_product (\n",
    "    product_key SERIAL PRIMARY KEY,\n",
    "    product_id VARCHAR(50) UNIQUE,\n",
    "    product_name VARCHAR(255),\n",
    "    ean_upc VARCHAR(50),\n",
    "    brand VARCHAR(100),\n",
    "    category VARCHAR(100),\n",
    "    segment VARCHAR(100),\n",
    "    is_active BOOLEAN DEFAULT TRUE\n",
    ");\n",
    "\n",
    "CREATE INDEX idx_product_id ON dim_product(product_id);\n",
    "CREATE INDEX idx_product_brand ON dim_product(brand);\n",
    "CREATE INDEX idx_product_category ON dim_product(category);\n",
    "\"\"\"\n",
    "\n",
    "ddl_statements.append((\"Create dim_product\", create_dim_product))\n",
    "\n",
    "# --- dim_date ---\n",
    "create_dim_date = \"\"\"\n",
    "CREATE TABLE dim_date (\n",
    "    date_key INTEGER PRIMARY KEY,\n",
    "    full_date DATE UNIQUE NOT NULL,\n",
    "    year INTEGER,\n",
    "    quarter INTEGER,\n",
    "    month INTEGER,\n",
    "    month_name VARCHAR(20),\n",
    "    week_of_year INTEGER,\n",
    "    day_of_month INTEGER,\n",
    "    day_of_week INTEGER,\n",
    "    day_name VARCHAR(20),\n",
    "    is_weekend BOOLEAN,\n",
    "    is_holiday BOOLEAN DEFAULT FALSE,\n",
    "    quarter_name VARCHAR(10),\n",
    "    year_month VARCHAR(10)\n",
    ");\n",
    "\n",
    "CREATE INDEX idx_date_full_date ON dim_date(full_date);\n",
    "CREATE INDEX idx_date_year_month ON dim_date(year, month);\n",
    "\"\"\"\n",
    "\n",
    "ddl_statements.append((\"Create dim_date\", create_dim_date))\n",
    "\n",
    "# --- dim_geography ---\n",
    "create_dim_geography = \"\"\"\n",
    "CREATE TABLE dim_geography (\n",
    "    geography_key SERIAL PRIMARY KEY,\n",
    "    address_id VARCHAR(50),\n",
    "    user_id VARCHAR(50),\n",
    "    country VARCHAR(100),\n",
    "    state VARCHAR(100),\n",
    "    city VARCHAR(100),\n",
    "    neighborhood VARCHAR(100),\n",
    "    postal_code VARCHAR(20),\n",
    "    street VARCHAR(255),\n",
    "    latitude DECIMAL(12,8),\n",
    "    longitude DECIMAL(12,8),\n",
    "    address_type VARCHAR(50),\n",
    "    is_default BOOLEAN\n",
    ");\n",
    "\n",
    "CREATE INDEX idx_geography_user_id ON dim_geography(user_id);\n",
    "CREATE INDEX idx_geography_country ON dim_geography(country);\n",
    "CREATE INDEX idx_geography_city ON dim_geography(city);\n",
    "\"\"\"\n",
    "\n",
    "ddl_statements.append((\"Create dim_geography\", create_dim_geography))\n",
    "\n",
    "# --- dim_order ---\n",
    "create_dim_order = \"\"\"\n",
    "CREATE TABLE dim_order (\n",
    "    order_key SERIAL PRIMARY KEY,\n",
    "    order_id VARCHAR(50) UNIQUE,\n",
    "    creation_date TIMESTAMP,\n",
    "    authorized_date TIMESTAMP,\n",
    "    invoiced_date TIMESTAMP,\n",
    "    order_status VARCHAR(50),\n",
    "    payment_method VARCHAR(50),\n",
    "    shipping_estimated_date DATE,\n",
    "    shipping_estimated_min DATE,\n",
    "    shipping_estimated_max DATE,\n",
    "    days_to_shipping INTEGER,\n",
    "    order_year INTEGER,\n",
    "    order_month INTEGER,\n",
    "    order_quarter INTEGER,\n",
    "    order_day_of_week INTEGER,\n",
    "    channel VARCHAR(50),\n",
    "    seller_id VARCHAR(50)\n",
    ");\n",
    "\n",
    "CREATE INDEX idx_order_id ON dim_order(order_id);\n",
    "CREATE INDEX idx_order_creation_date ON dim_order(creation_date);\n",
    "CREATE INDEX idx_order_status ON dim_order(order_status);\n",
    "\"\"\"\n",
    "\n",
    "ddl_statements.append((\"Create dim_order\", create_dim_order))\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CREATE FACT TABLE WITH INCREASED DECIMAL PRECISION\n",
    "# ==============================================================================\n",
    "\n",
    "create_fact_sales = \"\"\"\n",
    "CREATE TABLE fact_sales (\n",
    "    sale_id SERIAL PRIMARY KEY,\n",
    "    order_id VARCHAR(50),\n",
    "    customer_key INTEGER REFERENCES dim_customer(customer_key),\n",
    "    product_key INTEGER REFERENCES dim_product(product_key),\n",
    "    date_key INTEGER REFERENCES dim_date(date_key),\n",
    "    geography_key INTEGER REFERENCES dim_geography(geography_key),\n",
    "    order_key INTEGER REFERENCES dim_order(order_key),\n",
    "    -- Measures (DECIMAL(15,2) to handle large price values)\n",
    "    quantity INTEGER,\n",
    "    unit_price DECIMAL(15,2),\n",
    "    list_price DECIMAL(15,2),\n",
    "    selling_price DECIMAL(15,2),\n",
    "    discount_amount DECIMAL(15,2),\n",
    "    shipping_price DECIMAL(15,2),\n",
    "    total_amount DECIMAL(15,2),\n",
    "    is_gift BOOLEAN\n",
    ");\n",
    "\n",
    "-- Create indexes on foreign keys for join performance\n",
    "CREATE INDEX idx_fact_customer_key ON fact_sales(customer_key);\n",
    "CREATE INDEX idx_fact_product_key ON fact_sales(product_key);\n",
    "CREATE INDEX idx_fact_date_key ON fact_sales(date_key);\n",
    "CREATE INDEX idx_fact_geography_key ON fact_sales(geography_key);\n",
    "CREATE INDEX idx_fact_order_key ON fact_sales(order_key);\n",
    "CREATE INDEX idx_fact_order_id ON fact_sales(order_id);\n",
    "\"\"\"\n",
    "\n",
    "ddl_statements.append((\"Create fact_sales\", create_fact_sales))\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. EXECUTE ALL DDL STATEMENTS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Executing DDL statements...\\n\")\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        for description, sql in ddl_statements:\n",
    "            print(f\"  ‚öôÔ∏è  {description}...\")\n",
    "            conn.execute(text(sql))\n",
    "            conn.commit()\n",
    "            print(f\"     ‚úÖ {description} - SUCCESS\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéâ Star Schema created successfully in PostgreSQL!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Verify tables were created\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"\"\"\n",
    "            SELECT table_name \n",
    "            FROM information_schema.tables \n",
    "            WHERE table_schema = 'public' \n",
    "            AND table_type = 'BASE TABLE'\n",
    "            ORDER BY table_name;\n",
    "        \"\"\"))\n",
    "        tables = [row[0] for row in result]\n",
    "        \n",
    "        print(\"\\nüìã Tables created in database:\")\n",
    "        for table in tables:\n",
    "            print(f\"   ‚Ä¢ {table}\")\n",
    "             \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error creating tables: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Sql data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Inserting 'Unknown' records for missing data handling\n",
      "\n",
      "  ‚öôÔ∏è  Inserting Unknown Customer...\n",
      "     ‚úÖ Unknown Customer inserted\n",
      "  ‚öôÔ∏è  Inserting Unknown Product...\n",
      "     ‚úÖ Unknown Product inserted\n",
      "  ‚öôÔ∏è  Inserting Unknown Date...\n",
      "     ‚úÖ Unknown Date inserted\n",
      "  ‚öôÔ∏è  Inserting Unknown Geography...\n",
      "     ‚úÖ Unknown Geography inserted\n",
      "  ‚öôÔ∏è  Inserting Unknown Order...\n",
      "     ‚úÖ Unknown Order inserted\n",
      "\n",
      "======================================================================\n",
      "‚úÖ 'Unknown' records inserted successfully!\n",
      "======================================================================\n",
      "\n",
      "These records will be used for NULL foreign key references\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîß Inserting 'Unknown' records for missing data handling\\n\")\n",
    "\n",
    "# Insert special \"Unknown\" records with fixed IDs for each dimension\n",
    "# These will be used when foreign key lookups fail (missing data)\n",
    "\n",
    "unknown_inserts = []\n",
    "\n",
    "# Unknown Customer (customer_key will be = 1)\n",
    "unknown_customer = \"\"\"\n",
    "INSERT INTO dim_customer (user_id, birth_date, customer_age, gender, email, phone, is_active)\n",
    "VALUES ('UNKNOWN', NULL, NULL, 'Unknown', 'unknown@unknown.com', 'N/A', FALSE);\n",
    "\"\"\"\n",
    "unknown_inserts.append((\"Unknown Customer\", unknown_customer))\n",
    "\n",
    "# Unknown Product (product_key will be = 1)\n",
    "unknown_product = \"\"\"\n",
    "INSERT INTO dim_product (product_id, product_name, ean_upc, brand, category, segment, is_active)\n",
    "VALUES ('UNKNOWN', 'Unknown Product', 'N/A', 'Unknown', 'Unknown', 'Unknown', FALSE);\n",
    "\"\"\"\n",
    "unknown_inserts.append((\"Unknown Product\", unknown_product))\n",
    "\n",
    "# Unknown Date (date_key = 19000101 = January 1, 1900)\n",
    "unknown_date = \"\"\"\n",
    "INSERT INTO dim_date (date_key, full_date, year, quarter, month, month_name, week_of_year, \n",
    "                      day_of_month, day_of_week, day_name, is_weekend, quarter_name, year_month)\n",
    "VALUES (19000101, '1900-01-01', 1900, 1, 1, 'January', 1, 1, 1, 'Monday', FALSE, 'Q1', '1900-01');\n",
    "\"\"\"\n",
    "unknown_inserts.append((\"Unknown Date\", unknown_date))\n",
    "\n",
    "# Unknown Geography (geography_key will be = 1)\n",
    "unknown_geography = \"\"\"\n",
    "INSERT INTO dim_geography (address_id, user_id, country, state, city, neighborhood, \n",
    "                           postal_code, street, address_type, is_default)\n",
    "VALUES ('UNKNOWN', 'UNKNOWN', 'Unknown', 'Unknown', 'Unknown', 'Unknown', \n",
    "        'N/A', 'Unknown', 'Unknown', FALSE);\n",
    "\"\"\"\n",
    "unknown_inserts.append((\"Unknown Geography\", unknown_geography))\n",
    "\n",
    "# Unknown Order (order_key will be = 1)\n",
    "unknown_order = \"\"\"\n",
    "INSERT INTO dim_order (order_id, order_status, payment_method, channel)\n",
    "VALUES ('UNKNOWN', 'Unknown', 'Unknown', 'Unknown');\n",
    "\"\"\"\n",
    "unknown_inserts.append((\"Unknown Order\", unknown_order))\n",
    "\n",
    "# Execute inserts\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        for description, sql in unknown_inserts:\n",
    "            print(f\"  ‚öôÔ∏è  Inserting {description}...\")\n",
    "            conn.execute(text(sql))\n",
    "            conn.commit()\n",
    "            print(f\"     ‚úÖ {description} inserted\")\n",
    "            \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ 'Unknown' records inserted successfully!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nThese records will be used for NULL foreign key references\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error inserting unknown records: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading Data into Star Schema\n",
      "\n",
      "======================================================================\n",
      "Loading order:\n",
      "  1. dim_date (generated)\n",
      "  2. dim_customer\n",
      "  3. dim_product\n",
      "  4. dim_geography\n",
      "  5. dim_order\n",
      "  6. fact_sales (with lookups)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Loading Data into Star Schema\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Loading order:\")\n",
    "print(\"  1. dim_date (generated)\")\n",
    "print(\"  2. dim_customer\")\n",
    "print(\"  3. dim_product\")\n",
    "print(\"  4. dim_geography\")\n",
    "print(\"  5. dim_order\")\n",
    "print(\"  6. fact_sales (with lookups)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dim date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è∞ Generating dim_date dimension...\n",
      "\n",
      "Data date range: 2021-01-01 to 2022-11-03\n",
      "Generating 732 dates from 2020-12-02 to 2022-12-03\n",
      "\n",
      "Loading 732 records to dim_date...\n",
      "‚úÖ dim_date loaded: 732 records\n",
      "   Verified in database: 733 records\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. LOAD dim_date - Generate date dimension programmatically\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n‚è∞ Generating dim_date dimension...\\n\")\n",
    "\n",
    "# Find date range from the data\n",
    "date_columns_to_check = [\n",
    "    ordersList['creationDate'],\n",
    "    generalOrder['creationDate']\n",
    "]\n",
    "\n",
    "# Get min and max dates\n",
    "all_dates = pd.concat(date_columns_to_check, ignore_index=True)\n",
    "all_dates = pd.to_datetime(all_dates, errors='coerce')\n",
    "all_dates = all_dates.dropna()\n",
    "\n",
    "min_date = all_dates.min()\n",
    "max_date = all_dates.max()\n",
    "\n",
    "print(f\"Data date range: {min_date.date()} to {max_date.date()}\")\n",
    "\n",
    "# Generate full date range (extend a bit for safety)\n",
    "start_date = min_date - pd.DateOffset(days=30)  # 30 days before\n",
    "end_date = max_date + pd.DateOffset(days=30)    # 30 days after\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "print(f\"Generating {len(date_range)} dates from {date_range[0].date()} to {date_range[-1].date()}\\n\")\n",
    "\n",
    "# Build dim_date DataFrame\n",
    "dim_date_data = []\n",
    "for date in date_range:\n",
    "    dim_date_data.append({\n",
    "        'date_key': int(date.strftime('%Y%m%d')),\n",
    "        'full_date': date.date(),\n",
    "        'year': date.year,\n",
    "        'quarter': date.quarter,\n",
    "        'month': date.month,\n",
    "        'month_name': date.strftime('%B'),\n",
    "        'week_of_year': date.isocalendar()[1],\n",
    "        'day_of_month': date.day,\n",
    "        'day_of_week': date.dayofweek + 1,  # 1=Monday, 7=Sunday\n",
    "        'day_name': date.strftime('%A'),\n",
    "        'is_weekend': date.dayofweek >= 5,  # Saturday=5, Sunday=6\n",
    "        'is_holiday': False,  # Could be enhanced with holiday calendar\n",
    "        'quarter_name': f'Q{date.quarter}',\n",
    "        'year_month': date.strftime('%Y-%m')\n",
    "    })\n",
    "\n",
    "dim_date_df = pd.DataFrame(dim_date_data)\n",
    "\n",
    "# Load to database\n",
    "print(f\"Loading {len(dim_date_df)} records to dim_date...\")\n",
    "dim_date_df.to_sql('dim_date', engine, if_exists='append', index=False, method='multi', chunksize=1000)\n",
    "print(f\"‚úÖ dim_date loaded: {len(dim_date_df):,} records\")\n",
    "\n",
    "# Verify\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM dim_date;\"))\n",
    "    count = result.fetchone()[0]\n",
    "    print(f\"   Verified in database: {count:,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dim Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ Loading dim_customer dimension...\n",
      "\n",
      "Prepared 109,679 unique customers\n",
      "Loading to database...\n",
      "‚úÖ dim_customer loaded: 109,679 records\n",
      "   Verified in database: 109,678 records (excluding Unknown)\n",
      "\n",
      "   Sample records:\n",
      "     (1, 'UNKNOWN', None, 'Unknown')\n",
      "     (2, 'f1ace526-a249-4cec-b47d-d4b00c035d9b', None, 'Unknown')\n",
      "     (3, '3e3c5cf1-db7d-4718-9c32-597adc65ce36', 26, 'female')\n",
      "     (4, 'd3c42b55-c52e-4695-b12e-11ab0112a1fe', None, 'Unknown')\n",
      "     (5, 'eed5119f-c0d3-4316-809b-df59c9d69b06', None, 'Unknown')\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2. LOAD dim_customer\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüë§ Loading dim_customer dimension...\\n\")\n",
    "\n",
    "# Prepare customer dimension from individualCustomer\n",
    "dim_customer_prep = individualCustomer[['userId', 'birthDate', 'customer_age', 'gender', \n",
    "                                         'email', 'phone', 'rclastsessiondate']].copy()\n",
    "\n",
    "# Rename columns to match dimension schema\n",
    "dim_customer_prep.rename(columns={\n",
    "    'userId': 'user_id',\n",
    "    'birthDate': 'birth_date',\n",
    "    'rclastsessiondate': 'last_session_date'\n",
    "}, inplace=True)\n",
    "\n",
    "# Convert dates to proper format\n",
    "dim_customer_prep['birth_date'] = pd.to_datetime(dim_customer_prep['birth_date'], errors='coerce')\n",
    "dim_customer_prep['last_session_date'] = pd.to_datetime(dim_customer_prep['last_session_date'], errors='coerce')\n",
    "\n",
    "# Add business logic columns\n",
    "dim_customer_prep['is_active'] = dim_customer_prep['last_session_date'].notna()\n",
    "dim_customer_prep['first_purchase_date'] = None  # Could be calculated from orders\n",
    "\n",
    "# Remove duplicates on user_id\n",
    "dim_customer_prep = dim_customer_prep.drop_duplicates(subset=['user_id'], keep='first')\n",
    "\n",
    "# Handle NaN values\n",
    "dim_customer_prep['gender'] = dim_customer_prep['gender'].fillna('Unknown')\n",
    "dim_customer_prep['email'] = dim_customer_prep['email'].fillna('unknown@unknown.com')\n",
    "dim_customer_prep['phone'] = dim_customer_prep['phone'].fillna('N/A')\n",
    "\n",
    "print(f\"Prepared {len(dim_customer_prep):,} unique customers\")\n",
    "\n",
    "# Load to database\n",
    "print(f\"Loading to database...\")\n",
    "dim_customer_prep.to_sql('dim_customer', engine, if_exists='append', index=False, method='multi', chunksize=5000)\n",
    "print(f\"‚úÖ dim_customer loaded: {len(dim_customer_prep):,} records\")\n",
    "\n",
    "# Verify\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM dim_customer WHERE user_id != 'UNKNOWN';\"))\n",
    "    count = result.fetchone()[0]\n",
    "    print(f\"   Verified in database: {count:,} records (excluding Unknown)\")\n",
    "    \n",
    "    # Show sample\n",
    "    result = conn.execute(text(\"SELECT customer_key, user_id, customer_age, gender FROM dim_customer LIMIT 5;\"))\n",
    "    print(\"\\n   Sample records:\")\n",
    "    for row in result:\n",
    "        print(f\"     {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dim Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõí Loading dim_product dimension...\n",
      "\n",
      "Prepared 7,158 unique products\n",
      "Loading to database...\n",
      "‚úÖ dim_product loaded: 7,158 records\n",
      "   Verified in database: 7,158 records (excluding Unknown)\n",
      "\n",
      "   Sample records:\n",
      "     1 | UNKNOWN | Unknown Product... | Unknown | Unknown\n",
      "     2 | 1 | WC07001Q... | WHR | T-06 FREEZER\n",
      "     3 | 2 | WA1045Q... | WHR | T-05 AT\n",
      "     4 | 3 | WA2043Q... | WHR | T-05 AT\n",
      "     5 | 4 | WC10001Q... | WHR | T-06 FREEZER\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. LOAD dim_product\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüõí Loading dim_product dimension...\\n\")\n",
    "\n",
    "# Prepare product dimension from productCatalog\n",
    "dim_product_prep = productCatalog[['IdMaterial', 'MATERIAL', 'EAN_UPC', 'BRAND', \n",
    "                                    'CATEGORY_PROJECT', 'SEGMENT_DESC']].copy()\n",
    "\n",
    "# Rename columns to match dimension schema\n",
    "dim_product_prep.rename(columns={\n",
    "    'IdMaterial': 'product_id',\n",
    "    'MATERIAL': 'product_name',\n",
    "    'EAN_UPC': 'ean_upc',\n",
    "    'BRAND': 'brand',\n",
    "    'CATEGORY_PROJECT': 'category',\n",
    "    'SEGMENT_DESC': 'segment'\n",
    "}, inplace=True)\n",
    "\n",
    "# Add is_active column (all products in catalog are active)\n",
    "dim_product_prep['is_active'] = True\n",
    "\n",
    "# Handle NaN values\n",
    "dim_product_prep['product_id'] = dim_product_prep['product_id'].fillna('UNKNOWN')\n",
    "dim_product_prep['product_name'] = dim_product_prep['product_name'].fillna('Unknown Product')\n",
    "dim_product_prep['ean_upc'] = dim_product_prep['ean_upc'].fillna('N/A')\n",
    "dim_product_prep['brand'] = dim_product_prep['brand'].fillna('Unknown')\n",
    "dim_product_prep['category'] = dim_product_prep['category'].fillna('Unknown')\n",
    "dim_product_prep['segment'] = dim_product_prep['segment'].fillna('Unknown')\n",
    "\n",
    "# Remove duplicates\n",
    "dim_product_prep = dim_product_prep.drop_duplicates(subset=['product_id'], keep='first')\n",
    "\n",
    "print(f\"Prepared {len(dim_product_prep):,} unique products\")\n",
    "\n",
    "# Load to database\n",
    "print(f\"Loading to database...\")\n",
    "dim_product_prep.to_sql('dim_product', engine, if_exists='append', index=False, method='multi', chunksize=1000)\n",
    "print(f\"‚úÖ dim_product loaded: {len(dim_product_prep):,} records\")\n",
    "\n",
    "# Verify\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM dim_product WHERE product_id != 'UNKNOWN';\"))\n",
    "    count = result.fetchone()[0]\n",
    "    print(f\"   Verified in database: {count:,} records (excluding Unknown)\")\n",
    "    \n",
    "    # Show sample\n",
    "    result = conn.execute(text(\"SELECT product_key, product_id, product_name, brand, category FROM dim_product LIMIT 5;\"))\n",
    "    print(\"\\n   Sample records:\")\n",
    "    for row in result:\n",
    "        print(f\"     {row[0]} | {row[1]} | {row[2][:30]}... | {row[3]} | {row[4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dim Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìç Loading dim_geography dimension...\n",
      "\n",
      "Prepared 56,233 unique addresses\n",
      "  Valid coordinates: 24,277 (43.2%)\n",
      "\n",
      "Loading to database...\n",
      "‚úÖ dim_geography loaded: 56,233 records\n",
      "   Verified in database: 56,233 records (excluding Unknown)\n",
      "\n",
      "   Sample records:\n",
      "     (1, 'UNKNOWN', 'Unknown', 'Unknown', 'Unknown')\n",
      "     (2, '70f350f5-f62e-11ec-835d-0a8fb171123f', 'GUADALUPE', 'NUEVO LE√ìN', 'MEX')\n",
      "     (3, 'c21ef477-e9c4-11ec-835d-02978ed58bf1', 'SAN NICOL√ÅS DE LOS GARZA', 'NUEVO LE√ìN', 'MEX')\n",
      "     (4, '66021709-f670-11ec-835d-16b245a39a51', 'MONTERREY', 'NUEVO LE√ìN', 'MEX')\n",
      "     (5, '79e873e1-e2c7-11ec-835d-1205375cb899', 'XOCHIMILCO', 'CIUDAD DE M√âXICO', 'MEX')\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. LOAD dim_geography\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüìç Loading dim_geography dimension...\\n\")\n",
    "\n",
    "# Prepare geography dimension from customerAddress\n",
    "dim_geography_prep = customerAddress[['id', 'userId', 'country', 'state', 'city', \n",
    "                                            'neighborhood', 'postalCode', 'addressName', \n",
    "                                            'geoCoordinate']].copy()\n",
    "\n",
    "# Rename columns\n",
    "dim_geography_prep.rename(columns={\n",
    "    'id': 'address_id',\n",
    "    'userId': 'user_id',\n",
    "    'postalCode': 'postal_code',\n",
    "    'addressName': 'street',\n",
    "    'geoCoordinate': 'geoCoordinates'  # Rename for consistency with processing\n",
    "}, inplace=True)\n",
    "\n",
    "# Parse latitude/longitude from geoCoordinates (assuming format like [-23.5,-46.6])\n",
    "def parse_coordinates(coord_str):\n",
    "    try:\n",
    "        if pd.isna(coord_str):\n",
    "            return None, None\n",
    "        # Remove brackets and split\n",
    "        coords = str(coord_str).strip('[]').split(',')\n",
    "        if len(coords) == 2:\n",
    "            lat = float(coords[0])\n",
    "            lon = float(coords[1])\n",
    "            \n",
    "            # Validate coordinate ranges\n",
    "            # Latitude: -90 to 90\n",
    "            # Longitude: -180 to 180\n",
    "            if -90 <= lat <= 90 and -180 <= lon <= 180:\n",
    "                return lat, lon\n",
    "            else:\n",
    "                # Invalid range, return None\n",
    "                return None, None\n",
    "    except:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "dim_geography_prep[['latitude', 'longitude']] = dim_geography_prep['geoCoordinates'].apply(\n",
    "    lambda x: pd.Series(parse_coordinates(x))\n",
    ")\n",
    "\n",
    "# Drop original geoCoordinates column\n",
    "dim_geography_prep = dim_geography_prep.drop(columns=['geoCoordinates'])\n",
    "\n",
    "# Add additional columns\n",
    "dim_geography_prep['address_type'] = 'Residential'  # Default assumption\n",
    "dim_geography_prep['is_default'] = False  # Could be enhanced with business logic\n",
    "\n",
    "# Handle NaN values\n",
    "dim_geography_prep['address_id'] = dim_geography_prep['address_id'].fillna('UNKNOWN')\n",
    "dim_geography_prep['user_id'] = dim_geography_prep['user_id'].fillna('UNKNOWN')\n",
    "dim_geography_prep['country'] = dim_geography_prep['country'].fillna('Unknown')\n",
    "dim_geography_prep['state'] = dim_geography_prep['state'].fillna('Unknown')\n",
    "dim_geography_prep['city'] = dim_geography_prep['city'].fillna('Unknown')\n",
    "dim_geography_prep['neighborhood'] = dim_geography_prep['neighborhood'].fillna('Unknown')\n",
    "dim_geography_prep['postal_code'] = dim_geography_prep['postal_code'].fillna('N/A')\n",
    "dim_geography_prep['street'] = dim_geography_prep['street'].fillna('Unknown')\n",
    "\n",
    "# Remove duplicates on address_id\n",
    "dim_geography_prep = dim_geography_prep.drop_duplicates(subset=['address_id'], keep='first')\n",
    "\n",
    "# Log coordinate statistics\n",
    "valid_coords = dim_geography_prep[['latitude', 'longitude']].notna().all(axis=1).sum()\n",
    "print(f\"Prepared {len(dim_geography_prep):,} unique addresses\")\n",
    "print(f\"  Valid coordinates: {valid_coords:,} ({valid_coords/len(dim_geography_prep)*100:.1f}%)\")\n",
    "\n",
    "# Load to database\n",
    "print(f\"\\nLoading to database...\")\n",
    "dim_geography_prep.to_sql('dim_geography', engine, if_exists='append', index=False, method='multi', chunksize=5000)\n",
    "print(f\"‚úÖ dim_geography loaded: {len(dim_geography_prep):,} records\")\n",
    "\n",
    "# Verify\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM dim_geography WHERE address_id != 'UNKNOWN';\"))\n",
    "    count = result.fetchone()[0]\n",
    "    print(f\"   Verified in database: {count:,} records (excluding Unknown)\")\n",
    "    \n",
    "    # Show sample\n",
    "    result = conn.execute(text(\"SELECT geography_key, user_id, city, state, country FROM dim_geography LIMIT 5;\"))\n",
    "    print(\"\\n   Sample records:\")\n",
    "    for row in result:\n",
    "        print(f\"     {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dim Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Loading dim_order dimension...\n",
      "\n",
      "Prepared 61,475 unique orders\n",
      "  Source: ordersList (67,831 records) + generalOrder (59,310 records)\n",
      "\n",
      "Loading to database...\n",
      "‚úÖ dim_order loaded: 61,475 records\n",
      "   Verified in database: 61,475 records (excluding Unknown)\n",
      "\n",
      "   Top order statuses:\n",
      "     invoiced: 23,152 orders\n",
      "     handling: 18,374 orders\n",
      "     canceled: 9,093 orders\n",
      "     ready-for-handling: 8,222 orders\n",
      "     payment-pending: 1,064 orders\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 5. LOAD dim_order\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüì¶ Loading dim_order dimension...\\n\")\n",
    "\n",
    "# Select columns from ordersList\n",
    "orders_from_list = ordersList[[\n",
    "    'orderId', 'creationDate', 'authorizedDate', 'status', 'paymentNames',\n",
    "    'salesChannel', 'ShippingEstimatedDate', 'ShippingEstimatedDateMin', \n",
    "    'ShippingEstimatedDateMax', 'days_to_shipping'\n",
    "]].copy()\n",
    "\n",
    "# Select columns from generalOrder\n",
    "orders_from_general = generalOrder[[\n",
    "    'orderId', 'invoicedDate', 'order_year', 'order_month', \n",
    "    'order_quarter', 'order_dayofweek'\n",
    "]].copy()\n",
    "\n",
    "# Merge both sources\n",
    "orders_merged = orders_from_list.merge(\n",
    "    orders_from_general,\n",
    "    on='orderId',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Prepare order dimension\n",
    "dim_order_prep = orders_merged.copy()\n",
    "\n",
    "# Rename columns to match schema\n",
    "dim_order_prep.rename(columns={\n",
    "    'orderId': 'order_id',\n",
    "    'creationDate': 'creation_date',\n",
    "    'authorizedDate': 'authorized_date',\n",
    "    'invoicedDate': 'invoiced_date',\n",
    "    'status': 'order_status',\n",
    "    'paymentNames': 'payment_method',\n",
    "    'ShippingEstimatedDate': 'shipping_estimated_date',\n",
    "    'ShippingEstimatedDateMin': 'shipping_estimated_min',\n",
    "    'ShippingEstimatedDateMax': 'shipping_estimated_max',\n",
    "    'order_dayofweek': 'order_day_of_week',\n",
    "    'salesChannel': 'channel'\n",
    "}, inplace=True)\n",
    "\n",
    "# Add seller_id column (not available in source data)\n",
    "dim_order_prep['seller_id'] = 'Unknown'\n",
    "\n",
    "# Convert dates to proper datetime format\n",
    "dim_order_prep['creation_date'] = pd.to_datetime(dim_order_prep['creation_date'], errors='coerce')\n",
    "dim_order_prep['authorized_date'] = pd.to_datetime(dim_order_prep['authorized_date'], errors='coerce')\n",
    "dim_order_prep['invoiced_date'] = pd.to_datetime(dim_order_prep['invoiced_date'], errors='coerce')\n",
    "dim_order_prep['shipping_estimated_date'] = pd.to_datetime(dim_order_prep['shipping_estimated_date'], errors='coerce')\n",
    "dim_order_prep['shipping_estimated_min'] = pd.to_datetime(dim_order_prep['shipping_estimated_min'], errors='coerce')\n",
    "dim_order_prep['shipping_estimated_max'] = pd.to_datetime(dim_order_prep['shipping_estimated_max'], errors='coerce')\n",
    "\n",
    "# Handle NaN/missing values\n",
    "dim_order_prep['order_id'] = dim_order_prep['order_id'].fillna('UNKNOWN')\n",
    "dim_order_prep['order_status'] = dim_order_prep['order_status'].fillna('Unknown')\n",
    "dim_order_prep['payment_method'] = dim_order_prep['payment_method'].fillna('Unknown')\n",
    "dim_order_prep['channel'] = dim_order_prep['channel'].fillna('Unknown')\n",
    "\n",
    "# Remove duplicates on order_id (keep first occurrence)\n",
    "dim_order_prep = dim_order_prep.drop_duplicates(subset=['order_id'], keep='first')\n",
    "\n",
    "print(f\"Prepared {len(dim_order_prep):,} unique orders\")\n",
    "print(f\"  Source: ordersList ({len(orders_from_list):,} records) + generalOrder ({len(orders_from_general):,} records)\")\n",
    "\n",
    "# Load to database\n",
    "print(f\"\\nLoading to database...\")\n",
    "dim_order_prep.to_sql('dim_order', engine, if_exists='append', index=False, method='multi', chunksize=5000)\n",
    "print(f\"‚úÖ dim_order loaded: {len(dim_order_prep):,} records\")\n",
    "\n",
    "# Verify\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM dim_order WHERE order_id != 'UNKNOWN';\"))\n",
    "    count = result.fetchone()[0]\n",
    "    print(f\"   Verified in database: {count:,} records (excluding Unknown)\")\n",
    "    \n",
    "    # Show sample with stats\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        SELECT order_status, COUNT(*) as count \n",
    "        FROM dim_order \n",
    "        WHERE order_id != 'UNKNOWN'\n",
    "        GROUP BY order_status \n",
    "        ORDER BY count DESC \n",
    "        LIMIT 5;\n",
    "    \"\"\"))\n",
    "    print(\"\\n   Top order statuses:\")\n",
    "    for row in result:\n",
    "        print(f\"     {row[0]}: {row[1]:,} orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Investigation - Duplicates Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç INVESTIGACI√ìN: ¬øPOR QU√â EL REVENUE ES TAN ALTO?\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Verificar si generalOrder tiene duplicados despu√©s de la limpieza\n",
    "print(\"\\n1. Verificar duplicados en generalOrder:\")\n",
    "dup_count = generalOrder['orderId'].duplicated().sum()\n",
    "print(f\"   OrderIds duplicados: {dup_count:,}\")\n",
    "if dup_count == 0:\n",
    "    print(\"   ‚úÖ No hay duplicados - la limpieza funcion√≥\")\n",
    "else:\n",
    "    print(f\"   ‚ùå A√∫n hay {dup_count:,} duplicados!\")\n",
    "\n",
    "# 2. Analizar distribuci√≥n de precios\n",
    "print(\"\\n2. Estad√≠sticas de PRECIOS en productOrderDetail:\")\n",
    "print(\"\\n   SELLING PRICE:\")\n",
    "print(productOrderDetail['sellingPrice'].describe())\n",
    "\n",
    "print(\"\\n3. Top 10 productos m√°s caros:\")\n",
    "top10 = productOrderDetail.nlargest(10, 'sellingPrice')[['orderId', 'productId', 'sellingPrice', 'quantity']]\n",
    "for idx, row in top10.iterrows():\n",
    "    print(f\"   OrderId: {row['orderId']}, ProductId: {row['productId']}, Price: ${row['sellingPrice']:,.2f}, Qty: {row['quantity']}\")\n",
    "\n",
    "# 4. Calcular revenue total directamente\n",
    "print(\"\\n4. Revenue calculado directamente desde productOrderDetail:\")\n",
    "direct_revenue = (productOrderDetail['sellingPrice'] * productOrderDetail['quantity']).sum()\n",
    "print(f\"   Total: ${direct_revenue:,.2f}\")\n",
    "\n",
    "# 5. Verificar si el problema es conversi√≥n de moneda\n",
    "print(\"\\n5. ¬øLos precios est√°n en CENTAVOS en lugar de PESOS?\")\n",
    "sample_prices = productOrderDetail['sellingPrice'].head(20)\n",
    "print(f\"   Muestra de 20 precios: {sample_prices.tolist()}\")\n",
    "print(f\"   Si estos precios est√°n en centavos, dividir entre 100\")\n",
    "\n",
    "# 6. Comparar join counts\n",
    "print(\"\\n6. Verificar counts en el join:\")\n",
    "print(f\"   productOrderDetail: {len(productOrderDetail):,} rows\")\n",
    "print(f\"   generalOrder: {len(generalOrder):,} rows\")\n",
    "print(f\"   generalOrder unique orderIds: {generalOrder['orderId'].nunique():,}\")\n",
    "\n",
    "# Simular merge\n",
    "test_merge = productOrderDetail[['orderId']].merge(\n",
    "    generalOrder[['orderId', 'ClientId']], \n",
    "    on='orderId', \n",
    "    how='left'\n",
    ")\n",
    "print(f\"   Despu√©s del merge: {len(test_merge):,} rows\")\n",
    "print(f\"   Aumento: +{len(test_merge) - len(productOrderDetail):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí∞ Loading fact_sales (with dimension lookups)...\n",
      "\n",
      "Step 1: Creating lookup dictionaries from dimensions...\n",
      "  ‚úì Customer lookup: 109,680 mappings\n",
      "  ‚úì Product lookup: 7,159 mappings\n",
      "  ‚úì Geography lookup: 50,153 mappings\n",
      "  ‚úì Order lookup: 61,476 mappings\n",
      "\n",
      "  ‚úÖ All lookup dictionaries created\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 6. LOAD fact_sales - WITH LOOKUPS TO ALL DIMENSIONS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüí∞ Loading fact_sales (with dimension lookups)...\\n\")\n",
    "\n",
    "# Step 1: Create lookup dictionaries from dimensions\n",
    "print(\"Step 1: Creating lookup dictionaries from dimensions...\")\n",
    "\n",
    "# Customer lookup: user_id -> customer_key\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT user_id, customer_key FROM dim_customer;\"))\n",
    "    customer_lookup = {row[0]: row[1] for row in result}\n",
    "    unknown_customer_key = customer_lookup.get('UNKNOWN', 1)\n",
    "    print(f\"  ‚úì Customer lookup: {len(customer_lookup):,} mappings\")\n",
    "\n",
    "# Product lookup: product_id -> product_key\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT product_id, product_key FROM dim_product;\"))\n",
    "    product_lookup = {row[0]: row[1] for row in result}\n",
    "    unknown_product_key = product_lookup.get('UNKNOWN', 1)\n",
    "    print(f\"  ‚úì Product lookup: {len(product_lookup):,} mappings\")\n",
    "\n",
    "# Date lookup: full_date -> date_key (create a helper function)\n",
    "def get_date_key(date_value):\n",
    "    \"\"\"Convert date to YYYYMMDD integer format\"\"\"\n",
    "    try:\n",
    "        if pd.isna(date_value):\n",
    "            return 19000101  # Unknown date key\n",
    "        dt = pd.to_datetime(date_value)\n",
    "        return int(dt.strftime('%Y%m%d'))\n",
    "    except:\n",
    "        return 19000101  # Unknown date key\n",
    "\n",
    "# Geography lookup: user_id -> geography_key (get first address per user)\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        SELECT DISTINCT ON (user_id) user_id, geography_key \n",
    "        FROM dim_geography \n",
    "        WHERE user_id != 'UNKNOWN'\n",
    "        ORDER BY user_id, geography_key;\n",
    "    \"\"\"))\n",
    "    geography_lookup = {row[0]: row[1] for row in result}\n",
    "    unknown_geography_key = geography_lookup.get('UNKNOWN', 1)\n",
    "    print(f\"  ‚úì Geography lookup: {len(geography_lookup):,} mappings\")\n",
    "\n",
    "# Order lookup: order_id -> order_key\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT order_id, order_key FROM dim_order;\"))\n",
    "    order_lookup = {row[0]: row[1] for row in result}\n",
    "    unknown_order_key = order_lookup.get('UNKNOWN', 1)\n",
    "    print(f\"  ‚úì Order lookup: {len(order_lookup):,} mappings\")\n",
    "\n",
    "print(f\"\\n  ‚úÖ All lookup dictionaries created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Preparing fact_sales data from productOrderDetail...\n",
      "\n",
      "  Initial records from productOrderDetail: 87,609\n",
      "  After joining with generalOrder: 87,609\n",
      "  ‚úì Data prepared for dimension lookups\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Prepare fact_sales data\n",
    "print(\"\\nStep 2: Preparing fact_sales data from productOrderDetail...\\n\")\n",
    "\n",
    "# Select relevant columns from productOrderDetail\n",
    "fact_sales_prep = productOrderDetail[[\n",
    "    'orderId', 'productId', 'quantity', 'price', 'listPrice', \n",
    "    'sellingPrice', 'shippingPrice', 'isGift'\n",
    "]].copy()\n",
    "\n",
    "print(f\"  Initial records from productOrderDetail: {len(fact_sales_prep):,}\")\n",
    "\n",
    "# Join with generalOrder to get ClientId (user_id) and creationDate\n",
    "fact_sales_prep = fact_sales_prep.merge(\n",
    "    generalOrder[['orderId', 'ClientId', 'creationDate']],\n",
    "    on='orderId',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"  After joining with generalOrder: {len(fact_sales_prep):,}\")\n",
    "\n",
    "# Rename ClientId to user_id for clarity\n",
    "fact_sales_prep.rename(columns={'ClientId': 'user_id'}, inplace=True)\n",
    "\n",
    "# Convert creationDate to datetime\n",
    "fact_sales_prep['creationDate'] = pd.to_datetime(fact_sales_prep['creationDate'], errors='coerce')\n",
    "\n",
    "# Convert product_id to string\n",
    "fact_sales_prep['productId'] = fact_sales_prep['productId'].astype(str)\n",
    "\n",
    "print(f\"  ‚úì Data prepared for dimension lookups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Applying dimension lookups to create foreign keys...\n",
      "\n",
      "  ‚úì Final fact_sales records: 87,609\n",
      "  ‚úì Unique orders: 61,578\n",
      "  ‚úì Unique customers: 1,979\n",
      "  ‚úì Unique products: 1,225\n",
      "  ‚úì Date range: 19000101 to 20221103\n",
      "  ‚úì Total revenue: $4,190,403,906,678.00\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Apply dimension lookups to get foreign keys\n",
    "print(\"\\nStep 3: Applying dimension lookups to create foreign keys...\\n\")\n",
    "\n",
    "# Lookup customer_key\n",
    "fact_sales_prep['customer_key'] = fact_sales_prep['user_id'].map(customer_lookup).fillna(unknown_customer_key).astype(int)\n",
    "\n",
    "# Lookup product_key\n",
    "fact_sales_prep['product_key'] = fact_sales_prep['productId'].map(product_lookup).fillna(unknown_product_key).astype(int)\n",
    "\n",
    "# Lookup date_key\n",
    "fact_sales_prep['date_key'] = fact_sales_prep['creationDate'].apply(get_date_key)\n",
    "\n",
    "# Lookup geography_key (using user_id)\n",
    "fact_sales_prep['geography_key'] = fact_sales_prep['user_id'].map(geography_lookup).fillna(unknown_geography_key).astype(int)\n",
    "\n",
    "# Lookup order_key\n",
    "fact_sales_prep['order_key'] = fact_sales_prep['orderId'].map(order_lookup).fillna(unknown_order_key).astype(int)\n",
    "\n",
    "# Calculate discount_amount (listPrice - sellingPrice)\n",
    "fact_sales_prep['discount_amount'] = fact_sales_prep['listPrice'] - fact_sales_prep['sellingPrice']\n",
    "fact_sales_prep['discount_amount'] = fact_sales_prep['discount_amount'].clip(lower=0)  # No negative discounts\n",
    "\n",
    "# Calculate total_amount (sellingPrice * quantity + shippingPrice)\n",
    "fact_sales_prep['total_amount'] = (fact_sales_prep['sellingPrice'] * fact_sales_prep['quantity']) + fact_sales_prep['shippingPrice'].fillna(0)\n",
    "\n",
    "# Prepare final fact table with correct column names\n",
    "fact_sales_final = fact_sales_prep[[\n",
    "    'orderId', 'customer_key', 'product_key', 'date_key', 'geography_key', 'order_key',\n",
    "    'quantity', 'price', 'listPrice', 'sellingPrice', 'discount_amount', \n",
    "    'shippingPrice', 'total_amount', 'isGift'\n",
    "]].copy()\n",
    "\n",
    "# Rename columns to match schema\n",
    "fact_sales_final.rename(columns={\n",
    "    'orderId': 'order_id',\n",
    "    'price': 'unit_price',\n",
    "    'listPrice': 'list_price',\n",
    "    'sellingPrice': 'selling_price',\n",
    "    'shippingPrice': 'shipping_price',\n",
    "    'isGift': 'is_gift'\n",
    "}, inplace=True)\n",
    "\n",
    "# Handle missing values\n",
    "fact_sales_final['unit_price'] = fact_sales_final['unit_price'].fillna(0)\n",
    "fact_sales_final['list_price'] = fact_sales_final['list_price'].fillna(0)\n",
    "fact_sales_final['selling_price'] = fact_sales_final['selling_price'].fillna(0)\n",
    "fact_sales_final['shipping_price'] = fact_sales_final['shipping_price'].fillna(0)\n",
    "fact_sales_final['discount_amount'] = fact_sales_final['discount_amount'].fillna(0)\n",
    "fact_sales_final['total_amount'] = fact_sales_final['total_amount'].fillna(0)\n",
    "fact_sales_final['quantity'] = fact_sales_final['quantity'].fillna(1).astype(int)\n",
    "fact_sales_final['is_gift'] = fact_sales_final['is_gift'].fillna(False)\n",
    "\n",
    "# Report statistics\n",
    "print(f\"  ‚úì Final fact_sales records: {len(fact_sales_final):,}\")\n",
    "print(f\"  ‚úì Unique orders: {fact_sales_final['order_id'].nunique():,}\")\n",
    "print(f\"  ‚úì Unique customers: {fact_sales_final['customer_key'].nunique():,}\")\n",
    "print(f\"  ‚úì Unique products: {fact_sales_final['product_key'].nunique():,}\")\n",
    "print(f\"  ‚úì Date range: {fact_sales_final['date_key'].min()} to {fact_sales_final['date_key'].max()}\")\n",
    "print(f\"  ‚úì Total revenue: ${fact_sales_final['total_amount'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Loading fact_sales to database...\n",
      "  Loading 87,609 records in batches...\n",
      "\n",
      "  Progress: 100.0% (87,609 / 87,609 records)\n",
      "\n",
      "‚úÖ fact_sales loaded: 87,609 records\n",
      "   Verified in database: 87,609 records\n",
      "   Total revenue: $4,190,403,906,678.00\n",
      "\n",
      "   Sample records:\n",
      "     Sale 1 | Order: 1100000450614-01 | Customer: 1 | Product: 707 | Qty: 1 | Total: $1224900.00\n",
      "     Sale 2 | Order: 1100000450614-01 | Customer: 1 | Product: 707 | Qty: 1 | Total: $1224900.00\n",
      "     Sale 3 | Order: 1100031691608-01 | Customer: 1 | Product: 1090 | Qty: 1 | Total: $278000.00\n",
      "     Sale 4 | Order: 1100031691608-01 | Customer: 1 | Product: 1090 | Qty: 1 | Total: $278000.00\n",
      "     Sale 5 | Order: 1100183075120-01 | Customer: 1 | Product: 209 | Qty: 1 | Total: $2414900.00\n",
      "\n",
      "======================================================================\n",
      "üéâ ALL DATA LOADED SUCCESSFULLY INTO STAR SCHEMA!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Load to database\n",
    "print(\"\\nStep 4: Loading fact_sales to database...\")\n",
    "print(f\"  Loading {len(fact_sales_final):,} records in batches...\\n\")\n",
    "\n",
    "# Load in chunks to avoid memory issues\n",
    "chunk_size = 10000\n",
    "total_chunks = (len(fact_sales_final) + chunk_size - 1) // chunk_size\n",
    "\n",
    "for i in range(total_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, len(fact_sales_final))\n",
    "    chunk = fact_sales_final.iloc[start_idx:end_idx]\n",
    "    \n",
    "    chunk.to_sql('fact_sales', engine, if_exists='append', index=False, method='multi')\n",
    "    \n",
    "    progress = (i + 1) / total_chunks * 100\n",
    "    print(f\"  Progress: {progress:5.1f}% ({end_idx:,} / {len(fact_sales_final):,} records)\", end='\\r')\n",
    "\n",
    "print(f\"\\n\\n‚úÖ fact_sales loaded: {len(fact_sales_final):,} records\")\n",
    "\n",
    "# Verify\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM fact_sales;\"))\n",
    "    count = result.fetchone()[0]\n",
    "    print(f\"   Verified in database: {count:,} records\")\n",
    "    \n",
    "    # Calculate total revenue\n",
    "    result = conn.execute(text(\"SELECT SUM(total_amount) FROM fact_sales;\"))\n",
    "    total_revenue = result.fetchone()[0]\n",
    "    print(f\"   Total revenue: ${total_revenue:,.2f}\" if total_revenue else \"   Total revenue: $0.00\")\n",
    "    \n",
    "    # Show sample\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        SELECT sale_id, order_id, customer_key, product_key, quantity, total_amount \n",
    "        FROM fact_sales \n",
    "        LIMIT 5;\n",
    "    \"\"\"))\n",
    "    print(\"\\n   Sample records:\")\n",
    "    for row in result:\n",
    "        print(f\"     Sale {row[0]} | Order: {row[1]} | Customer: {row[2]} | Product: {row[3]} | Qty: {row[4]} | Total: ${row[5]:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ ALL DATA LOADED SUCCESSFULLY INTO STAR SCHEMA!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "piym5v0eu2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç VERIFICANDO DUPLICADOS EN generalOrder\n",
      "\n",
      "======================================================================\n",
      "\n",
      "1. Duplicados exactos (todas las columnas): 0\n",
      "\n",
      "2. OrderIds duplicados: 0\n",
      "\n",
      "   ‚úÖ No hay duplicados por orderId\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Verificar duplicados en generalOrder (datos \"limpios\")\n",
    "print(\"üîç VERIFICANDO DUPLICADOS EN generalOrder\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check 1: Duplicados totales (todas las columnas)\n",
    "total_dups = generalOrder.duplicated().sum()\n",
    "print(f\"\\n1. Duplicados exactos (todas las columnas): {total_dups:,}\")\n",
    "\n",
    "# Check 2: Duplicados por orderId (la clave de negocio)\n",
    "duplicate_orderids = generalOrder['orderId'].duplicated().sum()\n",
    "print(f\"\\n2. OrderIds duplicados: {duplicate_orderids:,}\")\n",
    "\n",
    "if duplicate_orderids > 0:\n",
    "    print(f\"\\n   ‚ö†Ô∏è PROBLEMA ENCONTRADO: generalOrder tiene {duplicate_orderids:,} orderIds duplicados!\")\n",
    "    \n",
    "    # Mostrar ejemplos\n",
    "    dup_ids = generalOrder[generalOrder['orderId'].duplicated(keep=False)]['orderId'].unique()[:5]\n",
    "    print(f\"\\n   Ejemplos de orderIds duplicados:\")\n",
    "    for oid in dup_ids:\n",
    "        count = (generalOrder['orderId'] == oid).sum()\n",
    "        print(f\"     - '{oid}': {count} veces\")\n",
    "    \n",
    "    # Estad√≠sticas\n",
    "    dup_df = generalOrder[generalOrder['orderId'].duplicated(keep=False)]\n",
    "    print(f\"\\n   Total de registros involucrados: {len(dup_df):,}\")\n",
    "    print(f\"   OrderIds √∫nicos duplicados: {dup_df['orderId'].nunique():,}\")\n",
    "else:\n",
    "    print(\"\\n   ‚úÖ No hay duplicados por orderId\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-ml-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
